---
title: Indexes
created: '2020-10-06T08:47:48.701Z'
modified: '2020-10-06T08:56:06.583Z'
---

# Indexes
## Full text search - basics
- Tokenization
  * is the process of splitting into individual words (maybe based on "spaces")
- Stop words
  * Ord som ikke bærer betydning i forhold til sammenhængen
- Stemming
  * is the process of reducing words to their root (stem)
